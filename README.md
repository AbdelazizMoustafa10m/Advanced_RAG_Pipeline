# Advanced RAG Pipeline

## Overview

**Advanced RAG** is a unified, highly configurable pipeline for Retrieval-Augmented Generation (RAG) that efficiently processes both code and technical documents (e.g., PDFs, DOCX). It leverages advanced document loaders, chunking, enrichment, and vector storage to enable high-quality semantic search and LLM-augmented querying over heterogeneous knowledge bases.

The pipeline is designed for:
- **Efficient processing of large codebases and document repositories**
- **Intelligent document routing and chunking** (using Docling and custom logic)
- **Metadata enrichment for both code and documents**
- **Flexible LLM integration for enrichment and query**
- **Seamless vector storage and retrieval**

## Key Features
- Specialized document loaders for PDFs (DoclingReader) and code files
- Grouping of document parts for efficient processing and to avoid redundant LLM calls
- Modular pipeline with detectors, processors, and enrichers
- Parallel processing support
- Configurable via Python dataclasses and environment variables
- Output includes enriched nodes for both LLM and embedding models

## Architecture

```
[data dir] → [EnhancedDirectoryLoader] → [DetectorService] → [DocumentTypeRouter]
   └─> [CodeProcessor] (for code)
   └─> [TechnicalDocumentProcessor] (for docs, e.g., PDF)
         └─> [DoclingChunker]
         └─> [DoclingMetadataGenerator]
   └─> [Metadata Enrichers]
→ [Vector Store] (Chroma, etc.)
→ [Query Engine]
```

- **main.py**: Entry point. Loads config, initializes pipeline, runs orchestrator, and saves output.
- **pipeline/orchestrator.py**: Core orchestrator that manages loading, routing, processing, and enrichment.
- **core/config.py**: Centralized configuration using dataclasses (for LLMs, loaders, detectors, etc.)
- **processors/**: Contains code and document processors, chunkers, and enrichers.
- **loaders/**: Specialized loaders for directories, code, and documents.
- **llm/**: LLM provider integration, prompt templates, and caching.
- **indexing/**: Vector store adapters and query engine.

## Setup

1. **Clone the repository**
2. *(Recommended)* Create and activate a Python virtual environment:
   ```bash
   python3 -m venv .venv
   source .venv/bin/activate
   ```
3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```
4. **Set up environment variables:**
   - Copy `.env.example` to `.env` and fill in API keys for LLM providers (if needed).

5. **Prepare your data:**
   - Place your code and document files in the `data/` directory (or configure another input directory).

## Usage

Run the main pipeline:
```bash
python main.py
```

- The pipeline processes all files from the input directory, routes and chunks them, enriches with LLMs (if enabled), and saves the results to `node_contents.txt`.
- Output statistics and summaries are printed to the console.

## Configuration

- All configuration is managed via `core/config.py` using Python dataclasses.
- You can customize:
  - Input/output directories
  - Parallelism
  - LLM providers and enrichment options
  - Vector store backend
  - Chunking and enrichment strategies

## Main Components
- **EnhancedDirectoryLoader**: Loads files and detects type (code vs. document)
- **DoclingReader**: Specialized PDF/document loader and chunker
- **CodeProcessor**: Splits and processes code files
- **TechnicalDocumentProcessor**: Handles technical docs (PDF, DOCX, etc.)
- **Metadata Generators**: Enrich nodes with metadata using LLMs
- **Vector Store**: Stores embeddings for semantic search (Chroma by default)

## Output
- All processed and enriched nodes are saved to `node_contents.txt` (and variants)
- Includes LLM and embedding model views for each node

## Extending
- Add new processors, enrichers, or loaders by extending the respective modules
- Integrate new LLMs or vector stores by updating config and adapters

## License
MIT License

---

*Generated by Cascade AI based on codebase analysis (April 2025)*
