# examples/simple_usage.py

import sys
import os
import logging
from multiprocessing import freeze_support

# --- Add project root to sys.path ---
# This is a more explicit approach to ensure Python can find our modules
import sys
import os

# Get the absolute path to the project root directory
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))

# Make sure the project root is in sys.path
if project_root not in sys.path:
    sys.path.insert(0, project_root)
    print(f"Added {project_root} to Python path")

# Verify the path contains our project directory
import pprint
print("Python path:")
pprint.pprint(sys.path)
# ------------------------------------

# --- Project Imports ---
try:
    # Using direct imports without the 'sidekick' prefix
    from sidekick.core.config import UnifiedConfig, ParallelConfig, DocumentType
    from sidekick.pipeline.orchestrator import PipelineOrchestrator
    # Optional: For indexing and querying demonstration
    from sidekick.indexing.vector_store import ChromaVectorStoreAdapter
    from sidekick.core.interfaces import IVectorStore
    
    # LlamaIndex imports using the correct module structure
    from llama_index.core.schema import MetadataMode
    from llama_index.core import Settings
    from llama_index.core.node_parser import SentenceSplitter
    # Optional embeddings
    # from llama_index.embeddings.huggingface import HuggingFaceEmbedding
except ImportError as e:
    print(f"Error importing project modules: {e}")
    print("Ensure you are running this script from the project root directory",
          "or have the 'sidekick' package installed correctly.")
    sys.exit(1)
# ----------------------

# --- Standard Library Imports ---
from dotenv import load_dotenv

# --- Global Setup ---
load_dotenv() # Load API keys from .env file

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout) # Ensure logs go to console
        # logging.FileHandler("rag_pipeline.log") # Optional: Log to file
    ]
)
# Reduce verbosity from libraries if needed
logging.getLogger('httpx').setLevel(logging.WARNING)
logging.getLogger('httpcore').setLevel(logging.WARNING)

logger = logging.getLogger(__name__)
# ------------------


# --- Main Execution ---
if __name__ == "__main__":
    # Apply multiprocessing guard
    freeze_support()
    logger.info("Starting Unified RAG Parser Example...")

    # 1. --- Configuration ---
    try:
        # Create configuration object
        # Adjust input_directory to point to the ACTUAL root
        # containing your 'code_repository' and 'technical_docs' subdirs
        config = UnifiedConfig(
            input_directory="./data", # Or "/path/to/your/data"
            # Example: Override LLM models if needed (ensure API key is in .env)
            # llm=LLMConfig(
            #     metadata_model="llama-3.1-8b-instant",
            #     query_model="llama-3.1-70b-versatile",
            #     api_key_env_var="GROQ_API_KEY" # Default, but explicit
            # ),
            # vector_store=VectorStoreConfig(embedding_model="BAAI/bge-small-en-v1.5"), # Example
            parallel=ParallelConfig(num_workers=4) # Use num_workers from config
        )
        logger.info("Configuration loaded successfully.")
        # You can print the config for verification: logger.debug(config)
    except ValueError as e:
        logger.error(f"Configuration validation error: {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Unexpected error loading config: {e}", exc_info=True)
        sys.exit(1)

    # Optional: Configure LlamaIndex Settings globally if needed (e.g., embedding model)
    # try:
    #     Settings.embed_model = HuggingFaceEmbedding(model_name=config.vector_store.embedding_model)
    #     logger.info(f"Set embedding model: {config.vector_store.embedding_model}")
    # except Exception as e:
    #     logger.error(f"Failed to set embedding model: {e}", exc_info=True)
    #     # Decide if this is critical or can proceed without embeddings

    # 2. --- Pipeline Execution ---
    processed_nodes = [] # Initialize
    try:
        logger.info("Initializing Pipeline Orchestrator...")
        orchestrator = PipelineOrchestrator(config)
        logger.info("Running the processing pipeline...")
        # The orchestrator now handles loading, detecting, routing, processing, enriching
        processed_nodes = orchestrator.run()
        logger.info(f"Pipeline finished. Processed {len(processed_nodes)} nodes.")
    except Exception as e:
        logger.error(f"Pipeline execution failed: {e}", exc_info=True)
        sys.exit(1)

    if not processed_nodes:
        logger.warning("No nodes were generated by the pipeline. Exiting.")
        sys.exit(0)

       # --- Output ---
    print("\n--- Last 35 Enriched Nodes ---")
    # Calculate the starting index to get the last 35 nodes
    start_idx = max(0, len(processed_nodes) - 35)
    
    # Print only the last 35 nodes
    for i in range(start_idx, len(processed_nodes)):
        node = processed_nodes[i]
        print(f"--- Node {i+1}/{len(processed_nodes)} ---")
        print(node.get_content(metadata_mode=MetadataMode.ALL))
        print("-" * 30)